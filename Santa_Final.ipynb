{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Imports\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_validate as cv\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0a39823131a387acf172ff050ff944274e007dc1"
      },
      "cell_type": "code",
      "source": "# RMSLE Metric formula\n\nNUM_FOLDS = 5\n\ndef rmsle_cv(model, X, y):\n    kf = KFold(NUM_FOLDS, shuffle=True, random_state=0).get_n_splits(X)\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "689bfa55d0687b7e702d092a49303f4893330392",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read in the data\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ny = train.loc[:,'target'].values\ntrain.drop(['ID','target'], axis=1, inplace=True)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0b3c0101a6767cd1706b17d94d1d1fd7c2ba494"
      },
      "cell_type": "markdown",
      "source": "# Quick/Dirty Regression Model (Baseline)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d7e6c050498e754c22f1de16fad3c3aaf60b57e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# # Linear Regression Model function\n\n# lr = LinearRegression()\n# rmsle_lr = rmsle_cv(lr, train, y)\n# print(np.mean(rmsle_lr), np.std(rmsle_lr))\n# # 1.7E15, 2.7E15",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "The quick dirty regression model got a root mean squared error of 1.7E15. Not only that but the algorithm was spitting out outrageous numbers suchs as negative prices or 10e10 prices. After doing a little research about this situation, two solutions was discovered. Decreasing the dimenstions or using a different model."
    },
    {
      "metadata": {
        "_uuid": "925f293a383b71e8c8f48c9b18d6c92a1cbd508e"
      },
      "cell_type": "markdown",
      "source": "# Scaling Features"
    },
    {
      "metadata": {
        "_uuid": "4494ccbb49fd76a96c3aede18c7549df9562f91e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scaling the features\n\nsc = StandardScaler()\ny = np.log1p(y)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2630f2bcf4e49bc4698a6e2fd8f850321370b71a"
      },
      "cell_type": "markdown",
      "source": "# Feature Selection"
    },
    {
      "metadata": {
        "_uuid": "5620954e7461f0f268f00699f52de9a7b2480151"
      },
      "cell_type": "markdown",
      "source": "## PCA"
    },
    {
      "metadata": {
        "_uuid": "543315fa8a143504fa77e06c4d2822887af64707"
      },
      "cell_type": "markdown",
      "source": "Before we perform PCA, we will remove some correlated features. Research showed that it provides a better PCA transformation when correlated features are removed. The reason is because if two variables are correlated, then their variance will be twice as high, which will consider it to be a higher component then it really is. [link: https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca] "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b7a7f8051b852e649634f0ee7339d7159a01bb7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Create correlation matrix\ncorr_matrix = train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c19323c48237b3324d4f0f907733b4cfc4fa5a8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Dropping correlating columns\n\ntrain.drop(train[to_drop], axis=1, inplace=True)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ae0f5ed3d2dae057fd5a80716d81e74a73043c7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scaling and PCA data on correlated data\n\ntrain_std_corr = sc.fit_transform(train)\n# pca_corr = PCA(n_components=None)\n# pca_corr.fit(train_std_corr)\n# var_exp_corr = pca_corr.explained_variance_ratio_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "316305108594477fd8cea4b6353cbb76a1ea7f33",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# # Plotting the variances\n\n# cum_var_exp = np.cumsum(var_exp_corr)\n# plt.bar(range(1,len(var_exp_corr)+1), var_exp_corr, alpha=0.5, align='center', label='individual explained variance')\n# plt.step(range(1,len(var_exp_corr)+1), cum_var_exp, where='mid', label='cumulative explained variance')\n# plt.ylabel('Explained variance ratio')\n# plt.xlabel('Principal components')\n# plt.legend(loc='best')\n# plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e707bc779bd5d0d16d5fafdc3ef07408e97ebe43",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Running PCA with 1000 components and transforming data\n\npca_corr = PCA(n_components=1000)\ntrain_pca_corr = pca_corr.fit_transform(train_std_corr)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bac39ae13e42776f8b6b0228ba20525f9db6980"
      },
      "cell_type": "markdown",
      "source": "# Models"
    },
    {
      "metadata": {
        "_uuid": "ee35801d457cfe6d312009a06bc0f895e6b36a89"
      },
      "cell_type": "markdown",
      "source": "## Linear Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3ea71838c7392d918588bc1daba4d343d285876d"
      },
      "cell_type": "code",
      "source": "# # Running a Quick Linear Regression on the transformed data\n\n# lr = LinearRegression()\n# rmsle_lr = rmsle_cv(lr, train_pca_corr, y)\n# print(np.mean(rmsle_lr), np.std(rmsle_lr))\n# # RMLSE: 3.8 STD: 0.13",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1e793e7d6ea53611b32fcada00f8ed2afa511dd"
      },
      "cell_type": "markdown",
      "source": "## Ridge Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "42f72d3435985d5a95814a6e20ee02c2dd8fdfef"
      },
      "cell_type": "code",
      "source": "# # Tuning alpha for Ridge Regression\n\n# alpha = [3, 2, 1.5, 1, .75]\n\n# ridge_scores = []\n# for i in alpha:\n#     ridge = Ridge(alpha=i,normalize=True)\n#     rmsle_ridge = rmsle_cv(ridge, train_pca_corr, y)\n#     ridge_scores.append((np.mean(rmsle_ridge), np.std(rmsle_ridge)))\n# ridge_scores",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dbd3198888fb7a2d0208ddd5288b009fafb387c0"
      },
      "cell_type": "code",
      "source": "# # Setting the parameters for the Ridge Regression and running the model\n\n# ridge = Ridge(alpha=1, normalize=True)\n# rmsle_ridge = rmsle_cv(ridge, train_pca_corr, y)\n# print(np.mean(rmsle_ridge), np.std(rmsle_ridge))\n# # Mean: 1.693, STD: 0.041",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "706be78a9a0cd2639024f9ffcfa2da265eb6874a"
      },
      "cell_type": "markdown",
      "source": "## Random Forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c78d250bbb897777a13cf418b11d7e26c1a8ffe7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# # Create the parameter grid based on the results of random search \n# param_grid = {'n_estimators': [200, 300],\n#               'min_samples_leaf': [4, 5],\n#               'min_samples_split': [6, 8]}\n\n# # Create a based model\n# rf = RandomForestRegressor()\n\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2)\n\n# # Fitting the training data\n# grid_search.fit(train_pca_corr, y)\n\n# # Retrieving the best parameters\n# grid_search.best_params_\n# # min_samples_leaf: 5, min_samples_split:6, n_estimators=200",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "033be182be622a1a641bbb2fc522995387557c9a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# rf = RandomForestRegressor(n_estimators=200, min_samples_leaf=5, min_samples_split=6)\n# rmsle_rf = rmsle_cv(rf, train_pca_corr, y)\n# np.mean(rmsle_rf), np.std(rmsle_rf)\n# # Mean: 1.522, STD: 0.0391",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fe4bf02f450d74dc49470333f2dd0298a13bf7e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# temp = pd.DataFrame(train_pca_corr)\n# non_target_column = list(temp.columns)\n\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=200, min_samples_leaf=5, min_samples_split=6)\n# rf.fit(temp,y)\n\n# %matplotlib inline\n# #do code to support model\n# #\"data\" is the X dataframe and model is the SKlearn object\n\n# feats = {}\n# for feature, importance in zip(non_target_column, rf.feature_importances_):\n#     feats[feature] = importance #add the name/value pair \n\n# import operator\n# sorted_feats = sorted(feats.items(), key=operator.itemgetter(1), reverse=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f611549cef9610fc535b8793e55627a59f174b8"
      },
      "cell_type": "markdown",
      "source": "## Ensembling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74804ea20ead32daa7784fed241416b762a09c1c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Function that ensembles models (credit goes to Shivang: https://www.kaggle.com/lightsalsa/ensemble-of-lgbm-and-xgb)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]  \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        predictions = np.column_stack([ model.predict(X) for model in self.models_ ])\n        return np.mean(predictions, axis=1)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "84d5224a5c13054293625b39ac5789c4e74f72c2"
      },
      "cell_type": "code",
      "source": "# Running an Ensemble\n\n# Setting up models\nrf = RandomForestRegressor(n_estimators=200, min_samples_leaf=5, min_samples_split=6)\nridge = Ridge(alpha=1, normalize=True)\n\n# Running the ensemble\naveraged_models = AveragingModels(models = (rf, ridge))\n\n# Printing the results of ensemble\nscore = rmsle_cv(averaged_models, train_pca_corr, y)\nprint(\"averaged score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2e302e688bce575d64856ed588f9d7788b32dc48"
      },
      "cell_type": "code",
      "source": "averaged_models = AveragingModels(models = (rf, ridge))",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35385eba8484ce5387dfe8e812f10beb7b8f1d30"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(averaged_models, train_pca_corr, y)\nprint(\"averaged score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "averaged score: 1.5722 (0.0434)\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "723369f32d510fcfe9d8b4f4b3591829f9a472ce"
      },
      "cell_type": "markdown",
      "source": "# Preparing for Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "93e34bd742ce5bc7143e2c17c0d0dc8643a592a9"
      },
      "cell_type": "code",
      "source": "# test_id = test['ID']\n# test.drop('ID', axis=1, inplace=True)\n# test.drop(test[to_drop], axis=1, inplace=True)\n# test_std = sc.transform(test)\n# test_pca = pca_corr.transform(test_std)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1918bd2121e1742ded2540e329871e0b5be56494"
      },
      "cell_type": "code",
      "source": "# sub = pd.DataFrame()\n# sub['ID'] = test_id\n# results = np.expm1(model.predict(test_pca).astype('float128'))\n# sub['target'] = results\n# sub['target'].clip(lower=0, inplace=True)\n# sub.to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "68340f8883074f666896a95146e7ab90927b77a4"
      },
      "cell_type": "code",
      "source": "# test_id = test['ID']\n# test.drop('ID', axis=1, inplace=True)\n# test.drop(test[to_drop], axis=1, inplace=True)\n# test_std = sc.transform(test)\n# test_pca = pca.transform(test_std)\n# sub = pd.DataFrame()\n# sub['ID'] = test_id\n# sub['target'] = np.expm1(alg.predict(test_pca))\n# sub.to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04ac7a7bc16a4501b55a81d1de440b840e157540"
      },
      "cell_type": "markdown",
      "source": "# Deep Learning"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "cf99c8dd3b81ff5b62ecff89c4315b8799a08829"
      },
      "cell_type": "code",
      "source": "# DL Imports\n\nimport keras\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Dropout, Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b96e7b6714f786d7aac85befe42dfbea35011a17"
      },
      "cell_type": "code",
      "source": "# RMSLE function for DL\n\ndef rmsle(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b390217d4e77240183811639e0074feb1873b82d"
      },
      "cell_type": "code",
      "source": "input_layer = Input(shape=(X_train.shape[1],))\nx = input_layer\nx = Dense(512, activation='linear')(x)\nx = Dropout(0.1)(x)\nx = Dense(512, activation='linear')(x)\nx = Dropout(0.1)(x)\nx = Dense(64, activation='linear')(x)\nx = Dense(1, activation='linear')(x)\ndl = Model(inputs=input_layer, outputs=x, name='model1')\ndl.compile(optimizer=Adam(lr=0.0001), loss='mean_squared_error', metrics=['mae', rmsle])\n\nbatch_size = 16\nepochs = 100\n\nhistory = dl.fit(X_train, y_train, \n          epochs=epochs,\n          batch_size=batch_size,\n          validation_data=(X_test, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d0b734e2ff6f884117e6658b70796e7a127c4d28"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nmae = history.history['mean_absolute_error']\nval_mae = history.history['val_mean_absolute_error']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(mae) + 1)\nplt.plot(epochs, mae, 'bo', label='Training acc')\nplt.plot(epochs, val_mae, 'b', label='Validation acc')\nplt.title('Training and validation MAE')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c129a8e0c4c0f5cd89cf48414ae031f1233839fc"
      },
      "cell_type": "code",
      "source": "# Applying cross validation to a deep learning algorithm\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\ncvscores = []\nfor train, test in kfold.split(train_pca_corr, y):\n  # create model\n    input_layer = Input(shape=(train_pca_corr[train].shape[1],))\n    x = input_layer\n    x = Dense(512, activation='linear')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(512, activation='linear')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(64, activation='linear')(x)\n    x = Dense(1, activation='linear')(x)\n    model = Model(inputs=input_layer, outputs=x, name='model1')\n    # Compile model\n    model.compile(loss=rmsle, optimizer='adam', metrics=[rmsle])\n    # Fit the model\n    model.fit(train_pca_corr[train], y[train], epochs=100, batch_size=16, verbose=0)\n    # evaluate the model\n    scores = model.evaluate(train_pca_corr[test], y[test], verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]))\n    cvscores.append(scores[1])\nprint(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}